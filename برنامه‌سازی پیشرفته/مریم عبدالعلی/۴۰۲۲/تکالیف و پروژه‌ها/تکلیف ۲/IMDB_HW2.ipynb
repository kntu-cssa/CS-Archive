{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Plot Similarity Analysis with KNN and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description:\n",
    "\n",
    "This project aims to analyze the similarity between movie plot summaries using a k-nearest neighbors (KNN) algorithm with TF-IDF (Term Frequency-Inverse Document Frequency) vectorization and cosine similarity, without relying on scikit-learn or NLTK libraries.\n",
    "\n",
    "\n",
    "## Project steps:\n",
    "\n",
    "### 1. Web Scraping: Employ web scraping techniques to collect movie plot summaries from https://www.imdb.com/chart/top/. Use libraries like Beautiful Soup and requests for extracting the relevant text data.\n",
    "\n",
    "`BeautifulSoup` is a Python library for pulling data out of HTML and XML files. It creates a parse tree from HTML or XML documents, allowing you to extract data easily.\n",
    "You should sends a GET request to the IMDb URL using the requests.get() function. It retrieves the HTML content of the web page and stores the response. Check whether the HTTP response status code is 200, which indicates that the request was successful and the web page was retrieved. Create a BeautifulSoup object to parse tree of the HTML document using html.parser. With the parse tree of the HTML document, you can use various methods and attributes provided by Beautiful Soup to extract specific elements or data from it. For example, you can find elements by tag name, class, id, etc., and extract their text content or attributes.\n",
    "\n",
    "### 2. Preprocessing: Clean and preprocess the scraped data to remove noise, HTML tags, and other irrelevant information. Implement data cleaning techniques such as punctuation removal, stop word removal, and special character elimination without utilizing NLTK. \n",
    "\n",
    "Stop words are common words that are often filtered out during text analysis because they occur frequently and do not carry significant semantic meaning (e.g., \"the\", \"and\", \"is\"). You can create a list of stop words and remove them from the text data. Stop word lists can be obtained from various sources or predefined libraries, or custom stop word lists can be created based on the specific context of the data. Special characters include non-alphanumeric characters such as symbols, emojis, or non-ASCII characters that may not be relevant to the analysis or processing.\n",
    "You can use regular expressions or string manipulation functions to filter out special characters from the text data. Regular expressions can be used to match and replace specific patterns of characters or symbols.\n",
    "\n",
    "### 3. TF-IDF Vectorization: Implement TF-IDF vectorization to convert the cleaned movie plot summaries into numerical feature vectors. Develop custom functions or algorithms to compute the TF-IDF scores for each term in the corpus without relying on scikit-learn.\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) vectorization is a widely used technique in natural language processing (NLP) and information retrieval for converting textual data into numerical feature vectors. TF-IDF vectorization captures the importance of each term (word) in a document relative to its occurrence frequency across a collection of documents. Let's delve into the details of TF-IDF vectorization:\n",
    "\n",
    "##### Term Frequency (TF):\n",
    "- **Definition**: Term Frequency (TF) measures the frequency of a term (word) within a document relative to the total number of terms in the document. It reflects how often a term occurs within a specific document.\n",
    "- **Calculation**: TF is calculated as the ratio of the number of occurrences of a term \\( t \\) in a document \\( d \\) to the total number of terms in the document.\n",
    "\n",
    " $$ \\text{TF}(t, d) = \\frac{\\text{Number of occurrences of term } t \\text{ in document } d}{\\text{Total number of terms in document } d} $$\n",
    "\n",
    "- **Example**: If the word \"movie\" appears 5 times in a document with a total of 100 terms, then the TF score for \"movie\" in that document would be $\\frac{5}{100} = 0.05. $\n",
    "\n",
    "##### Inverse Document Frequency (IDF):\n",
    "- **Definition**: Inverse Document Frequency (IDF) measures the rarity or uniqueness of a term across a collection of documents. It highlights terms that are more discriminative or informative by penalizing terms that appear frequently in many documents.\n",
    "- **Calculation**: IDF is calculated as the logarithm of the ratio of the total number of documents \\( N \\) to the number of documents containing the term \\( t \\), with the result optionally adjusted to prevent division by zero and for smoothing purposes.\n",
    "$$ \\text{IDF}(t) = \\log \\left( \\frac{N}{\\text{Number of documents containing term } t} \\right) + 1 $$\n",
    "- **Example**: If there are 1,000 documents in total and the word \"movie\" appears in 100 documents, then the IDF score for \"movie\" would be $ \\log \\left( \\frac{1000}{100} \\right) + 1 = \\log(10) + 1 = 2 + 1 = 3 $.\n",
    "\n",
    "##### TF-IDF Vectorization:\n",
    "- **Definition**: TF-IDF vectorization combines the TF and IDF scores to represent each document as a numerical vector. It assigns higher weights to terms that are frequent within a document (high TF) but rare across the entire document collection (high IDF), indicating their importance or relevance.\n",
    "- **Calculation**: The TF-IDF score for a term \\( t \\) in a document \\( d \\) is calculated as the product of its TF and IDF scores.\n",
    "  $$ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t) $$\n",
    "- **Example**: If the TF score for \"movie\" in a document is 0.05 and the IDF score for \"movie\" is 3, then the TF-IDF score for \"movie\" in that document would be $0.05 \\times 3 = 0.15 $.\n",
    "\n",
    "\n",
    "TF-IDF vectorization is a powerful technique for converting text data into numerical feature vectors, enabling the application of various machine learning algorithms for tasks such as text classification, clustering, and information retrieval. It emphasizes the importance of terms in documents while considering their distribution across the document collection, thereby capturing the semantic relevance of terms in textual data.\n",
    "\n",
    "### 4. KNN Model Training: Train a k-nearest neighbors (KNN) model using the TF-IDF vectors of the movie plot summaries. \n",
    "\n",
    "- What is KNN?: K-nearest neighbors (KNN) is a simple, non-parametric machine learning algorithm used for classification and regression tasks. It predicts the class or value of a new data point by considering the majority class or average value of its nearest neighbors. In our case, which is similarity analysis, instead of using labeled data to predict the class of new instances, KNN focuses on finding the nearest neighbors of each data point in the feature space\n",
    "\n",
    "- Using TF-IDF Vectors with KNN:\n",
    "Feature Matrix: The TF-IDF vectors of the movie plot summaries serve as the feature matrix for training the KNN model. Each row of the feature matrix corresponds to a movie plot summary, and each column represents a term in the vocabulary.\n",
    "Distance Metric: In the context of similarity analysis, cosine similarity is commonly used as the distance metric for KNN. It measures the cosine of the angle between two vectors and ranges from -1 (opposite directions) to 1 (same direction), with higher values indicating greater similarity. \n",
    "\n",
    "- Similarity analysis using KNN: Given a new plot summary represented as a TF-IDF vector, the model identifies the k nearest neighbors in the feature space and returns their indices or similarity scores.\n",
    "\n",
    "### 5. Querying Similar Movies: Create functions to accept a new movie plot summary as input. Convert the input plot summary into a TF-IDF vector using the custom TF-IDF implementation. Utilize the trained KNN model to identify the k nearest neighbors (similar movies) to the input plot summary based on cosine similarity.\n",
    "\n",
    "\n",
    "### (BONUS) 6. compare the performance of your custom implementation with implementations that utilize NLTK and scikit-learn libraries.\n",
    "\n",
    "\n",
    "By developing custom implementations for TF-IDF and KNN, the project seeks to analyze the content similarity between movie plot summaries and identify similar movies based on their textual content. The findings may have implications for content recommendation systems, movie recommendation engines, and text similarity analysis across diverse domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
